{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt4VAdIKdZYL",
        "outputId": "39dbdbd2-2aed-4235-8689-2bac6d570b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visdom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDHKmC-nb1Ek",
        "outputId": "d1ce0620-41bd-4a06-f0c3-d3b083fa8498"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.3.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.8/dist-packages (from visdom) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from visdom) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from visdom) (2.25.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.8/dist-packages (from visdom) (6.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from visdom) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from visdom) (3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from visdom) (7.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->visdom) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->visdom) (4.0.0)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.3-py3-none-any.whl size=1417107 sha256=91cd26141c941f2aac24785f0e2a21ad48793cbef8bdf874deb3e761306c7686\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/73/32/0bbe55d2dccb9d80d3f020f474c15a5a1eef232817dcebe776\n",
            "Successfully built visdom\n",
            "Installing collected packages: websocket-client, jsonpointer, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.3 visdom-0.2.3 websocket-client-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# vis = visdom.Visdom()\n",
        "# vis.server.start()\n",
        "\n",
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "\n",
        "validation_loss_list = []\n",
        "validation_accuracy_list = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 80\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                            train=False, \n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=100, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=100, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# 3x3 convolution\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=stride, padding=1, bias=False)\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(3, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0]//2)\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1]//2, 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2]//2, 2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "    \n",
        "\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For updating learning rate\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct  = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # vis.line(X=np.array([epoch]), Y=np.array([loss]), win='loss', update='append')\n",
        "\n",
        "        train_loss_list.append(loss.item())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "        train_accuracy_list.append(100 * correct / total)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Decay learning rate\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        curr_lr /= 3\n",
        "        update_lr(optimizer, curr_lr)\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        validation_loss_list.append(loss.item())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        validation_accuracy_list.append(100 * correct / total)\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')\n",
        "x1 = range(0, 40000)\n",
        "x2 = range(0, 40000)\n",
        "y1 = train_accuracy_list\n",
        "y2 = train_loss_list\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(x1, y1, 'o-')\n",
        "plt.title('Train accuracy vs. epoches')\n",
        "plt.ylabel('Train accuracy')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(x2, y2, '.-')\n",
        "plt.xlabel('Train loss vs. epoches')\n",
        "plt.ylabel('Train loss')\n",
        "plt.show()\n",
        "plt.savefig(\"train_accuracy_loss.jpg\")\n",
        "\n",
        "x3 = range(0, 40000)\n",
        "x4 = range(0, 40000)\n",
        "y3 = train_accuracy_list\n",
        "y4 = train_loss_list\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(x3, y3, 'o-')\n",
        "plt.title('Validation accuracy vs. epoches')\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(x4, y4, '.-')\n",
        "plt.xlabel('Validation loss vs. epoches')\n",
        "plt.ylabel('validation loss')\n",
        "plt.show()\n",
        "plt.savefig(\"validation_accuracy_loss.jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biHzVxULIaLd",
        "outputId": "f58ab8f9-8149-4c53-9e7d-cb233c7f7d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/80], Step [100/500] Loss: 1.6400\n",
            "Epoch [1/80], Step [200/500] Loss: 1.4182\n",
            "Epoch [1/80], Step [300/500] Loss: 1.3409\n",
            "Epoch [1/80], Step [400/500] Loss: 1.2229\n",
            "Epoch [1/80], Step [500/500] Loss: 1.2522\n",
            "Epoch [2/80], Step [100/500] Loss: 1.0456\n",
            "Epoch [2/80], Step [200/500] Loss: 0.9661\n",
            "Epoch [2/80], Step [300/500] Loss: 1.1344\n",
            "Epoch [2/80], Step [400/500] Loss: 1.0979\n",
            "Epoch [2/80], Step [500/500] Loss: 1.0262\n",
            "Epoch [3/80], Step [100/500] Loss: 1.0895\n",
            "Epoch [3/80], Step [200/500] Loss: 1.1771\n",
            "Epoch [3/80], Step [300/500] Loss: 1.1157\n",
            "Epoch [3/80], Step [400/500] Loss: 0.9317\n",
            "Epoch [3/80], Step [500/500] Loss: 1.1548\n",
            "Epoch [4/80], Step [100/500] Loss: 0.8779\n",
            "Epoch [4/80], Step [200/500] Loss: 0.7054\n",
            "Epoch [4/80], Step [300/500] Loss: 0.6664\n",
            "Epoch [4/80], Step [400/500] Loss: 0.9595\n",
            "Epoch [4/80], Step [500/500] Loss: 1.0374\n",
            "Epoch [5/80], Step [100/500] Loss: 0.7833\n",
            "Epoch [5/80], Step [200/500] Loss: 0.6971\n",
            "Epoch [5/80], Step [300/500] Loss: 0.7856\n",
            "Epoch [5/80], Step [400/500] Loss: 0.9036\n",
            "Epoch [5/80], Step [500/500] Loss: 1.0236\n",
            "Epoch [6/80], Step [100/500] Loss: 0.7488\n",
            "Epoch [6/80], Step [200/500] Loss: 1.1031\n",
            "Epoch [6/80], Step [300/500] Loss: 0.9196\n",
            "Epoch [6/80], Step [400/500] Loss: 0.9747\n",
            "Epoch [6/80], Step [500/500] Loss: 0.6583\n",
            "Epoch [7/80], Step [100/500] Loss: 0.6143\n",
            "Epoch [7/80], Step [200/500] Loss: 0.8590\n",
            "Epoch [7/80], Step [300/500] Loss: 0.6410\n",
            "Epoch [7/80], Step [400/500] Loss: 0.8484\n",
            "Epoch [7/80], Step [500/500] Loss: 0.8612\n",
            "Epoch [8/80], Step [100/500] Loss: 0.8543\n",
            "Epoch [8/80], Step [200/500] Loss: 0.7845\n",
            "Epoch [8/80], Step [300/500] Loss: 0.6147\n",
            "Epoch [8/80], Step [400/500] Loss: 0.7278\n",
            "Epoch [8/80], Step [500/500] Loss: 0.6661\n",
            "Epoch [9/80], Step [100/500] Loss: 0.6290\n",
            "Epoch [9/80], Step [200/500] Loss: 0.5600\n",
            "Epoch [9/80], Step [300/500] Loss: 0.7436\n",
            "Epoch [9/80], Step [400/500] Loss: 0.7192\n",
            "Epoch [9/80], Step [500/500] Loss: 0.6266\n",
            "Epoch [10/80], Step [100/500] Loss: 0.4991\n",
            "Epoch [10/80], Step [200/500] Loss: 0.8463\n",
            "Epoch [10/80], Step [300/500] Loss: 0.5372\n",
            "Epoch [10/80], Step [400/500] Loss: 0.6224\n",
            "Epoch [10/80], Step [500/500] Loss: 0.7138\n",
            "Epoch [11/80], Step [100/500] Loss: 0.6681\n",
            "Epoch [11/80], Step [200/500] Loss: 0.7122\n",
            "Epoch [11/80], Step [300/500] Loss: 0.7948\n",
            "Epoch [11/80], Step [400/500] Loss: 0.5600\n",
            "Epoch [11/80], Step [500/500] Loss: 0.6521\n",
            "Epoch [12/80], Step [100/500] Loss: 0.6490\n",
            "Epoch [12/80], Step [200/500] Loss: 0.5864\n",
            "Epoch [12/80], Step [300/500] Loss: 0.5466\n",
            "Epoch [12/80], Step [400/500] Loss: 0.5607\n",
            "Epoch [12/80], Step [500/500] Loss: 0.6620\n",
            "Epoch [13/80], Step [100/500] Loss: 0.5003\n",
            "Epoch [13/80], Step [200/500] Loss: 0.6014\n",
            "Epoch [13/80], Step [300/500] Loss: 0.6643\n",
            "Epoch [13/80], Step [400/500] Loss: 0.7724\n",
            "Epoch [13/80], Step [500/500] Loss: 0.6575\n",
            "Epoch [14/80], Step [100/500] Loss: 0.5907\n",
            "Epoch [14/80], Step [200/500] Loss: 0.5322\n",
            "Epoch [14/80], Step [300/500] Loss: 0.5764\n",
            "Epoch [14/80], Step [400/500] Loss: 0.5865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
        "model.load_state_dict(torch.load(\"resnet.ckpt\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEMEWI_6MNau",
        "outputId": "54f55ec4-5d8d-4e2e-e906-bc6e0becfd60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights = [] \n",
        "conv_layers = [] \n",
        "model_children = list(model.children())\n",
        "\n",
        "# counter to keep count of the conv layers\n",
        "counter = 0 \n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children)):\n",
        "    if type(model_children[i]) == nn.Conv2d:\n",
        "        counter += 1\n",
        "        model_weights.append(model_children[i].weight)\n",
        "        conv_layers.append(model_children[i])\n",
        "    elif type(model_children[i]) == nn.Sequential:\n",
        "        for j in range(len(model_children[i])):\n",
        "            for child in model_children[i][j].children():\n",
        "                if type(child) == nn.Conv2d:\n",
        "                    counter += 1\n",
        "                    model_weights.append(child.weight)\n",
        "                    conv_layers.append(child)\n",
        "print(f\"Total convolutional layers: {counter}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6BEnJFeMrF6",
        "outputId": "d8cda262-54eb-4854-9537-2229f289233c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total convolutional layers: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the first conv layer filters\n",
        "plt.figure(figsize=(20, 17))\n",
        "for i, filter in enumerate(model_weights[1]):\n",
        "    plt.subplot(4, 4, i+1) # we have 3x3 filters and total of 16 (see printed shapes)\n",
        "    plt.imshow(filter[0, :, :].detach().cpu().numpy(), cmap='viridis')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('filter1.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "0Xd8OrnBNLA9",
        "outputId": "dacf73c9-5f34-4df3-f571-852247d30f45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1224 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZFjYNlqgxfaP"
      }
    }
  ]
}